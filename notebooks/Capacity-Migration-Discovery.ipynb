{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "11063a06",
      "metadata": {},
      "source": [
        "# Microsoft Fabric Capacity Migration Discovery\n",
        "\n",
        "## Overview\n",
        "This notebook performs comprehensive discovery of Power BI Premium/Embedded capacities and identifies potential blockers for migration to Microsoft Fabric capacities.\n",
        "\n",
        "## What This Notebook Does\n",
        "1. ‚úÖ Validates prerequisites (admin access, permissions)\n",
        "2. üìä Collects data from all capacities, workspaces, and items\n",
        "3. üîç Analyzes migration blockers and compatibility issues\n",
        "4. üíæ Stores data in a Fabric Lakehouse\n",
        "5. üìà Creates interactive Power BI report for analysis\n",
        "\n",
        "## Prerequisites\n",
        "- **Tenant Administrator** or **Capacity Administrator** permissions\n",
        "- XMLA Read/Write enabled on capacities\n",
        "- Fabric workspace with Lakehouse creation permissions\n",
        "\n",
        "## Last Updated\n",
        "November 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fcf8b24",
      "metadata": {},
      "source": [
        "## Step 1: Install Required Libraries\n",
        "\n",
        "Install semantic-link and semantic-link-labs for Fabric integration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79da2263",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install semantic-link-labs --upgrade --quiet\n",
        "print(\"‚úì Libraries installed successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c6acd46",
      "metadata": {},
      "source": [
        "## Step 2: Configure Parameters\n",
        "\n",
        "Set the names for your lakehouse, semantic model, and report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dfe138f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration Parameters\n",
        "semantic_model_name = \"Capacity Migration Analysis\"\n",
        "report_name = \"Capacity Migration Report\"\n",
        "lakehouse = \"CapacityMigrationLH\"\n",
        "\n",
        "# Validate parameters\n",
        "if not all([semantic_model_name, report_name, lakehouse]):\n",
        "    raise ValueError(\"‚ùå Error: All parameters (semantic_model_name, report_name, lakehouse) must be set\")\n",
        "\n",
        "print(\"‚úì Configuration validated:\")\n",
        "print(f\"  ‚Ä¢ Lakehouse: {lakehouse}\")\n",
        "print(f\"  ‚Ä¢ Semantic Model: {semantic_model_name}\")\n",
        "print(f\"  ‚Ä¢ Report: {report_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66892f0d",
      "metadata": {},
      "source": [
        "## Step 3: Import Libraries and Validate Permissions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d0474d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sempy\n",
        "import sempy.fabric as fabric\n",
        "import sempy_labs as labs\n",
        "import sempy_labs.admin as labs_admin\n",
        "import sempy_labs.report as labs_report\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"‚úì Libraries imported successfully\")\n",
        "\n",
        "# Validate tenant admin access\n",
        "try:\n",
        "    test_capacities = labs_admin._capacities._list_capacities_meta()\n",
        "    print(f\"‚úì Tenant admin access confirmed\")\n",
        "    print(f\"  ‚Ä¢ Found {len(test_capacities)} capacities accessible\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: Requires tenant/capacity admin permissions\")\n",
        "    print(f\"  ‚Ä¢ Details: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d7792c",
      "metadata": {},
      "source": [
        "## Step 4: Create or Verify Lakehouse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "050eaa13",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if Lakehouse exists, otherwise create it\n",
        "try:\n",
        "    df_items = fabric.list_items()\n",
        "    lakehouse_exists = df_items['Display Name'].eq(lakehouse).any()\n",
        "    \n",
        "    if lakehouse_exists:\n",
        "        lhid = df_items[\n",
        "            (df_items['Display Name'].eq(lakehouse)) & \n",
        "            (df_items['Type'].eq('Lakehouse'))\n",
        "        ].iloc[0, 0]\n",
        "        print(f\"‚úì Lakehouse '{lakehouse}' already exists (ID: {lhid})\")\n",
        "    else:\n",
        "        print(f\"Creating new Lakehouse '{lakehouse}'...\")\n",
        "        lhid = fabric.create_lakehouse(lakehouse)\n",
        "        print(f\"‚úì Lakehouse created successfully (ID: {lhid})\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creating/verifying Lakehouse: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35655873",
      "metadata": {},
      "source": [
        "## Step 5: Collect Capacity Data\n",
        "\n",
        "Gathering all capacity information including SKU, region, and state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5546bd71",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Collecting capacity data...\")\n",
        "\n",
        "try:\n",
        "    # Get all capacities\n",
        "    df_capacities = labs_admin._capacities._list_capacities_meta()\n",
        "    \n",
        "    # Convert Admins column to string to avoid type conflicts\n",
        "    if 'Admins' in df_capacities.columns:\n",
        "        df_capacities['Admins'] = df_capacities['Admins'].apply(\n",
        "            lambda x: json.dumps(x) if x is not None else None\n",
        "        )\n",
        "    \n",
        "    # Add a record for non-premium workspaces\n",
        "    new_record = pd.DataFrame([{\n",
        "        \"Capacity Id\": \"-1\",\n",
        "        \"Capacity Name\": \"Non Premium (Shared)\",\n",
        "        \"Sku\": \"Shared\",\n",
        "        \"Region\": \"N/A\",\n",
        "        \"State\": \"Active\",\n",
        "        \"Admins\": json.dumps([\"N/A\"])\n",
        "    }])\n",
        "    \n",
        "    df_capacities = pd.concat([df_capacities, new_record], ignore_index=True)\n",
        "    \n",
        "    print(f\"‚úì Collected {len(df_capacities) - 1} Premium/Fabric capacities\")\n",
        "    print(f\"\\nCapacity Summary:\")\n",
        "    print(df_capacities.groupby('Sku').size().to_string())\n",
        "    \n",
        "    # Save to Lakehouse\n",
        "    labs.save_as_delta_table(\n",
        "        dataframe=df_capacities,\n",
        "        delta_table_name=\"Capacities\",\n",
        "        write_mode=\"overwrite\",\n",
        "        lakehouse=lakehouse\n",
        "    )\n",
        "    print(\"\\n‚úì Capacities data saved to Lakehouse\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error collecting capacity data: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b01145d",
      "metadata": {},
      "source": [
        "## Step 6: Collect Workspace Data\n",
        "\n",
        "Gathering all workspace information and capacity assignments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "079b501e",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Collecting workspace data...\")\n",
        "\n",
        "try:\n",
        "    # Get all workspaces\n",
        "    all_workspaces_df = labs_admin.list_workspaces()\n",
        "    \n",
        "    # Fill null Capacity Ids with -1 (non-premium)\n",
        "    all_workspaces_df['Capacity Id'] = all_workspaces_df['Capacity Id'].fillna(\"-1\")\n",
        "    \n",
        "    print(f\"‚úì Collected {len(all_workspaces_df)} workspaces\")\n",
        "    \n",
        "    # Workspace breakdown\n",
        "    premium_count = (all_workspaces_df['Capacity Id'] != \"-1\").sum()\n",
        "    shared_count = (all_workspaces_df['Capacity Id'] == \"-1\").sum()\n",
        "    \n",
        "    print(f\"  ‚Ä¢ Premium/Fabric workspaces: {premium_count}\")\n",
        "    print(f\"  ‚Ä¢ Shared workspaces: {shared_count}\")\n",
        "    \n",
        "    # Save to Lakehouse\n",
        "    labs.save_as_delta_table(\n",
        "        dataframe=all_workspaces_df,\n",
        "        delta_table_name=\"Workspaces\",\n",
        "        write_mode=\"overwrite\",\n",
        "        lakehouse=lakehouse\n",
        "    )\n",
        "    print(\"‚úì Workspaces data saved to Lakehouse\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error collecting workspace data: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "197f2463",
      "metadata": {},
      "source": [
        "## Step 7: Collect Workspace Items\n",
        "\n",
        "Gathering all items (reports, datasets, dataflows, etc.) across all workspaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b36bd287",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Collecting workspace items...\")\n",
        "\n",
        "try:\n",
        "    # Get all items across all workspaces\n",
        "    df_items = labs_admin.list_items()\n",
        "    \n",
        "    print(f\"‚úì Collected {len(df_items)} workspace items\")\n",
        "    print(f\"\\nItem Type Breakdown:\")\n",
        "    print(df_items['Type'].value_counts().to_string())\n",
        "    \n",
        "    # Save to Lakehouse\n",
        "    labs.save_as_delta_table(\n",
        "        dataframe=df_items,\n",
        "        delta_table_name=\"WorkspaceItems\",\n",
        "        write_mode=\"overwrite\",\n",
        "        lakehouse=lakehouse\n",
        "    )\n",
        "    print(\"\\n‚úì Workspace items data saved to Lakehouse\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error collecting workspace items: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9680d1c",
      "metadata": {},
      "source": [
        "## Step 8: Collect Semantic Model Data\n",
        "\n",
        "Gathering detailed information about all semantic models (datasets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e973f4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Collecting semantic model data...\")\n",
        "\n",
        "try:\n",
        "    # Get all semantic models\n",
        "    df_semantic_models = labs_admin.list_datasets()\n",
        "    \n",
        "    # Convert complex fields to JSON strings for storage\n",
        "    df_semantic_models[\"Upstream Datasets\"] = df_semantic_models[\"Upstream Datasets\"].apply(\n",
        "        lambda x: json.dumps(x) if x and x != \"[]\" else None\n",
        "    )\n",
        "    df_semantic_models[\"Users\"] = df_semantic_models[\"Users\"].apply(\n",
        "        lambda x: json.dumps(x) if x and x != \"[]\" else None\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úì Collected {len(df_semantic_models)} semantic models\")\n",
        "    \n",
        "    # Storage mode breakdown\n",
        "    if 'Target Storage Mode' in df_semantic_models.columns:\n",
        "        print(f\"\\nStorage Mode Breakdown:\")\n",
        "        print(df_semantic_models['Target Storage Mode'].value_counts().to_string())\n",
        "    \n",
        "    # Save to Lakehouse\n",
        "    labs.save_as_delta_table(\n",
        "        dataframe=df_semantic_models,\n",
        "        delta_table_name=\"SemanticModels\",\n",
        "        write_mode=\"overwrite\",\n",
        "        lakehouse=lakehouse\n",
        "    )\n",
        "    print(\"\\n‚úì Semantic models data saved to Lakehouse\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error collecting semantic model data: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc55ac47",
      "metadata": {},
      "source": [
        "## Step 9: Migration Blocker Analysis\n",
        "\n",
        "Analyzing potential blockers for migration from Premium to Fabric capacities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8395af6",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"MIGRATION READINESS ASSESSMENT\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "\n",
        "blockers = []\n",
        "warnings = []\n",
        "info_items = []\n",
        "\n",
        "# ============================================\n",
        "# 1. Check for Embedded (EM) SKUs\n",
        "# ============================================\n",
        "em_skus = df_capacities[\n",
        "    df_capacities['Sku'].str.startswith('EM', na=False) |\n",
        "    df_capacities['Sku'].str.startswith('A', na=False)  # Azure A SKUs\n",
        "]\n",
        "if not em_skus.empty:\n",
        "    for _, cap in em_skus.iterrows():\n",
        "        if cap['Sku'].startswith('EM'):\n",
        "            blockers.append(\n",
        "                f\"Embedded capacity '{cap['Capacity Name']}' (SKU: {cap['Sku']}) - \"\n",
        "                f\"EM SKUs not supported in Fabric. Migrate to F-SKUs.\"\n",
        "            )\n",
        "        elif cap['Sku'].startswith('A'):\n",
        "            info_items.append(\n",
        "                f\"Azure capacity '{cap['Capacity Name']}' (SKU: {cap['Sku']}) - \"\n",
        "                f\"Azure Embedded can migrate to Fabric F-SKUs.\"\n",
        "            )\n",
        "\n",
        "# ============================================\n",
        "# 2. Check for Premium P SKUs (ready for migration)\n",
        "# ============================================\n",
        "p_skus = df_capacities[df_capacities['Sku'].str.startswith('P', na=False)]\n",
        "if not p_skus.empty:\n",
        "    info_items.append(\n",
        "        f\"{len(p_skus)} Premium P-SKU capacities found - Ready for Fabric migration\"\n",
        "    )\n",
        "\n",
        "# ============================================\n",
        "# 3. Check for cross-region workspaces\n",
        "# ============================================\n",
        "capacity_regions = df_capacities[\n",
        "    df_capacities['Region'] != 'N/A'\n",
        "]['Region'].unique()\n",
        "\n",
        "if len(capacity_regions) > 1:\n",
        "    warnings.append(\n",
        "        f\"Multiple regions detected: {', '.join(capacity_regions)}. \"\n",
        "        f\"Fabric capacities are region-specific. Plan migrations within same region.\"\n",
        "    )\n",
        "\n",
        "# ============================================\n",
        "# 4. Check for Dataflows Gen1\n",
        "# ============================================\n",
        "dataflows_gen1 = df_items[df_items['Type'] == 'Dataflow']\n",
        "if not dataflows_gen1.empty:\n",
        "    warnings.append(\n",
        "        f\"{len(dataflows_gen1)} Dataflow Gen1 artifacts found. \"\n",
        "        f\"Consider upgrading to Dataflow Gen2 or Data Pipelines in Fabric.\"\n",
        "    )\n",
        "\n",
        "# ============================================\n",
        "# 5. Check for Paginated Reports\n",
        "# ============================================\n",
        "paginated = df_items[df_items['Type'] == 'PaginatedReport']\n",
        "if not paginated.empty:\n",
        "    warnings.append(\n",
        "        f\"{len(paginated)} Paginated Reports found. \"\n",
        "        f\"Ensure Fabric capacity has paginated report workload enabled.\"\n",
        "    )\n",
        "\n",
        "# ============================================\n",
        "# 6. Check for Large Models (Premium Files)\n",
        "# ============================================\n",
        "if 'Target Storage Mode' in df_semantic_models.columns:\n",
        "    large_models = df_semantic_models[\n",
        "        df_semantic_models['Target Storage Mode'] == 'PremiumFiles'\n",
        "    ]\n",
        "    if not large_models.empty:\n",
        "        warnings.append(\n",
        "            f\"{len(large_models)} Large Models (>10GB) detected. \"\n",
        "            f\"Verify target Fabric capacity size supports these models.\"\n",
        "        )\n",
        "\n",
        "# ============================================\n",
        "# 7. Check for Inactive Workspaces\n",
        "# ============================================\n",
        "if 'State' in all_workspaces_df.columns:\n",
        "    inactive = all_workspaces_df[all_workspaces_df['State'] != 'Active']\n",
        "    if not inactive.empty:\n",
        "        warnings.append(\n",
        "            f\"{len(inactive)} inactive workspaces found (States: {inactive['State'].unique()}). \"\n",
        "            f\"Review and clean up before migration.\"\n",
        "        )\n",
        "\n",
        "# ============================================\n",
        "# 8. Check for Models with RLS\n",
        "# ============================================\n",
        "if 'Is Effective Identity Required' in df_semantic_models.columns:\n",
        "    models_with_rls = df_semantic_models[\n",
        "        df_semantic_models['Is Effective Identity Required'] == True\n",
        "    ]\n",
        "    if not models_with_rls.empty:\n",
        "        info_items.append(\n",
        "            f\"{len(models_with_rls)} semantic models with Row-Level Security (RLS). \"\n",
        "            f\"Test RLS behavior after migration, especially with DirectLake.\"\n",
        "        )\n",
        "\n",
        "# ============================================\n",
        "# 9. Check for Dashboards\n",
        "# ============================================\n",
        "dashboards = df_items[df_items['Type'] == 'Dashboard']\n",
        "if not dashboards.empty:\n",
        "    info_items.append(\n",
        "        f\"{len(dashboards)} Dashboards found. Dashboards migrate with their tiles and data sources.\"\n",
        "    )\n",
        "\n",
        "# ============================================\n",
        "# 10. Check for Deployment Pipelines\n",
        "# ============================================\n",
        "if 'Pipeline Id' in all_workspaces_df.columns:\n",
        "    pipelines = all_workspaces_df[all_workspaces_df['Pipeline Id'].notna()]\n",
        "    if not pipelines.empty:\n",
        "        info_items.append(\n",
        "            f\"{len(pipelines)} workspaces use Deployment Pipelines. \"\n",
        "            f\"Pipelines are supported in Fabric - verify configuration post-migration.\"\n",
        "        )\n",
        "\n",
        "# ============================================\n",
        "# Display Results\n",
        "# ============================================\n",
        "print(\"\\n\")\n",
        "\n",
        "if blockers:\n",
        "    print(\"üõë CRITICAL BLOCKERS (Must resolve before migration):\")\n",
        "    print(\"=\" * 70)\n",
        "    for i, blocker in enumerate(blockers, 1):\n",
        "        print(f\"{i}. {blocker}\")\n",
        "    print()\n",
        "\n",
        "if warnings:\n",
        "    print(\"‚ö†Ô∏è  WARNINGS (Review and plan accordingly):\")\n",
        "    print(\"=\" * 70)\n",
        "    for i, warning in enumerate(warnings, 1):\n",
        "        print(f\"{i}. {warning}\")\n",
        "    print()\n",
        "\n",
        "if info_items:\n",
        "    print(\"‚ÑπÔ∏è  INFORMATIONAL (For your awareness):\")\n",
        "    print(\"=\" * 70)\n",
        "    for i, info in enumerate(info_items, 1):\n",
        "        print(f\"{i}. {info}\")\n",
        "    print()\n",
        "\n",
        "if not blockers and not warnings:\n",
        "    print(\"‚úÖ MIGRATION READY\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"No critical blockers or warnings detected.\")\n",
        "    print(\"Your environment appears ready for Fabric migration.\")\n",
        "    print()\n",
        "\n",
        "# ============================================\n",
        "# Save Analysis Results\n",
        "# ============================================\n",
        "analysis_results = pd.DataFrame({\n",
        "    'Analysis Date': [datetime.now()],\n",
        "    'Total Capacities': [len(df_capacities) - 1],\n",
        "    'Total Workspaces': [len(all_workspaces_df)],\n",
        "    'Total Items': [len(df_items)],\n",
        "    'Total Models': [len(df_semantic_models)],\n",
        "    'Blocker Count': [len(blockers)],\n",
        "    'Warning Count': [len(warnings)],\n",
        "    'Blockers': [json.dumps(blockers)],\n",
        "    'Warnings': [json.dumps(warnings)],\n",
        "    'Info': [json.dumps(info_items)]\n",
        "})\n",
        "\n",
        "labs.save_as_delta_table(\n",
        "    dataframe=analysis_results,\n",
        "    delta_table_name=\"MigrationAnalysis\",\n",
        "    write_mode=\"overwrite\",\n",
        "    lakehouse=lakehouse\n",
        ")\n",
        "\n",
        "print(\"‚úì Analysis results saved to Lakehouse (Table: MigrationAnalysis)\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9547cd04",
      "metadata": {},
      "source": [
        "## Step 10: Create Semantic Model\n",
        "\n",
        "Creating a DirectLake semantic model for analysis and reporting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a68081f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Creating semantic model...\")\n",
        "\n",
        "# Define the BIM (Business Intelligence Model) structure\n",
        "bim_model = {\n",
        "    \"compatibilityLevel\": 1604,\n",
        "    \"model\": {\n",
        "        \"culture\": \"en-US\",\n",
        "        \"dataAccessOptions\": {\n",
        "            \"legacyRedirects\": True,\n",
        "            \"returnErrorValuesAsNull\": True\n",
        "        },\n",
        "        \"defaultPowerBIDataSourceVersion\": \"powerBI_V3\",\n",
        "        \"sourceQueryCulture\": \"en-US\",\n",
        "        \"tables\": [\n",
        "            {\n",
        "                \"name\": \"Capacities\",\n",
        "                \"columns\": [\n",
        "                    {\"name\": \"Capacity Id\", \"dataType\": \"string\", \"sourceColumn\": \"Capacity Id\"},\n",
        "                    {\"name\": \"Capacity Name\", \"dataType\": \"string\", \"sourceColumn\": \"Capacity Name\"},\n",
        "                    {\"name\": \"Sku\", \"dataType\": \"string\", \"sourceColumn\": \"Sku\"},\n",
        "                    {\"name\": \"Region\", \"dataType\": \"string\", \"sourceColumn\": \"Region\"},\n",
        "                    {\"name\": \"State\", \"dataType\": \"string\", \"sourceColumn\": \"State\"}\n",
        "                ],\n",
        "                \"measures\": [\n",
        "                    {\n",
        "                        \"name\": \"Total Capacities\",\n",
        "                        \"expression\": \"COUNTROWS(Capacities)\",\n",
        "                        \"formatString\": \"#,0\"\n",
        "                    }\n",
        "                ],\n",
        "                \"partitions\": [\n",
        "                    {\n",
        "                        \"name\": \"Capacities\",\n",
        "                        \"mode\": \"directLake\",\n",
        "                        \"source\": {\n",
        "                            \"type\": \"entity\",\n",
        "                            \"entityName\": \"Capacities\",\n",
        "                            \"expressionSource\": \"DatabaseQuery\",\n",
        "                            \"schemaName\": \"dbo\"\n",
        "                        }\n",
        "                    }\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"Workspaces\",\n",
        "                \"columns\": [\n",
        "                    {\"name\": \"Id\", \"dataType\": \"string\", \"sourceColumn\": \"Id\"},\n",
        "                    {\"name\": \"Name\", \"dataType\": \"string\", \"sourceColumn\": \"Name\"},\n",
        "                    {\"name\": \"Capacity Id\", \"dataType\": \"string\", \"sourceColumn\": \"Capacity Id\"},\n",
        "                    {\"name\": \"Type\", \"dataType\": \"string\", \"sourceColumn\": \"Type\"},\n",
        "                    {\"name\": \"State\", \"dataType\": \"string\", \"sourceColumn\": \"State\"}\n",
        "                ],\n",
        "                \"measures\": [\n",
        "                    {\n",
        "                        \"name\": \"Total Workspaces\",\n",
        "                        \"expression\": \"COUNTROWS(Workspaces)\",\n",
        "                        \"formatString\": \"#,0\"\n",
        "                    }\n",
        "                ],\n",
        "                \"partitions\": [\n",
        "                    {\n",
        "                        \"name\": \"Workspaces\",\n",
        "                        \"mode\": \"directLake\",\n",
        "                        \"source\": {\n",
        "                            \"type\": \"entity\",\n",
        "                            \"entityName\": \"Workspaces\",\n",
        "                            \"expressionSource\": \"DatabaseQuery\",\n",
        "                            \"schemaName\": \"dbo\"\n",
        "                        }\n",
        "                    }\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"WorkspaceItems\",\n",
        "                \"columns\": [\n",
        "                    {\"name\": \"Item Id\", \"dataType\": \"string\", \"sourceColumn\": \"Item Id\"},\n",
        "                    {\"name\": \"Item Name\", \"dataType\": \"string\", \"sourceColumn\": \"Item Name\"},\n",
        "                    {\"name\": \"Type\", \"dataType\": \"string\", \"sourceColumn\": \"Type\"},\n",
        "                    {\"name\": \"Workspace Id\", \"dataType\": \"string\", \"sourceColumn\": \"Workspace Id\"}\n",
        "                ],\n",
        "                \"measures\": [\n",
        "                    {\n",
        "                        \"name\": \"Total Items\",\n",
        "                        \"expression\": \"COUNTROWS(WorkspaceItems)\",\n",
        "                        \"formatString\": \"#,0\"\n",
        "                    }\n",
        "                ],\n",
        "                \"partitions\": [\n",
        "                    {\n",
        "                        \"name\": \"WorkspaceItems\",\n",
        "                        \"mode\": \"directLake\",\n",
        "                        \"source\": {\n",
        "                            \"type\": \"entity\",\n",
        "                            \"entityName\": \"WorkspaceItems\",\n",
        "                            \"expressionSource\": \"DatabaseQuery\",\n",
        "                            \"schemaName\": \"dbo\"\n",
        "                        }\n",
        "                    }\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"SemanticModels\",\n",
        "                \"columns\": [\n",
        "                    {\"name\": \"Dataset Id\", \"dataType\": \"string\", \"sourceColumn\": \"Dataset Id\"},\n",
        "                    {\"name\": \"Dataset Name\", \"dataType\": \"string\", \"sourceColumn\": \"Dataset Name\"},\n",
        "                    {\"name\": \"Workspace Id\", \"dataType\": \"string\", \"sourceColumn\": \"Workspace Id\"},\n",
        "                    {\"name\": \"Target Storage Mode\", \"dataType\": \"string\", \"sourceColumn\": \"Target Storage Mode\"}\n",
        "                ],\n",
        "                \"measures\": [\n",
        "                    {\n",
        "                        \"name\": \"Total Models\",\n",
        "                        \"expression\": \"COUNTROWS(SemanticModels)\",\n",
        "                        \"formatString\": \"#,0\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"name\": \"Large Models\",\n",
        "                        \"expression\": \"CALCULATE(COUNTROWS(SemanticModels), SemanticModels[Target Storage Mode] = \\\"PremiumFiles\\\")\",\n",
        "                        \"formatString\": \"#,0\"\n",
        "                    }\n",
        "                ],\n",
        "                \"partitions\": [\n",
        "                    {\n",
        "                        \"name\": \"SemanticModels\",\n",
        "                        \"mode\": \"directLake\",\n",
        "                        \"source\": {\n",
        "                            \"type\": \"entity\",\n",
        "                            \"entityName\": \"SemanticModels\",\n",
        "                            \"expressionSource\": \"DatabaseQuery\",\n",
        "                            \"schemaName\": \"dbo\"\n",
        "                        }\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        \"relationships\": [\n",
        "            {\n",
        "                \"name\": \"Workspace-Capacity\",\n",
        "                \"fromTable\": \"Workspaces\",\n",
        "                \"fromColumn\": \"Capacity Id\",\n",
        "                \"toTable\": \"Capacities\",\n",
        "                \"toColumn\": \"Capacity Id\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"Items-Workspace\",\n",
        "                \"fromTable\": \"WorkspaceItems\",\n",
        "                \"fromColumn\": \"Workspace Id\",\n",
        "                \"toTable\": \"Workspaces\",\n",
        "                \"toColumn\": \"Id\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"Models-Workspace\",\n",
        "                \"fromTable\": \"SemanticModels\",\n",
        "                \"fromColumn\": \"Workspace Id\",\n",
        "                \"toTable\": \"Workspaces\",\n",
        "                \"toColumn\": \"Id\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "try:\n",
        "    # Create semantic model from BIM definition\n",
        "    labs.create_semantic_model_from_bim(\n",
        "        dataset=semantic_model_name,\n",
        "        bim_file=bim_model,\n",
        "        lakehouse=lakehouse\n",
        "    )\n",
        "    print(f\"‚úì Semantic model '{semantic_model_name}' created successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creating semantic model: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05fe175c",
      "metadata": {},
      "source": [
        "## Step 11: Update Model Connection and Create Report\n",
        "\n",
        "Connecting the semantic model to the lakehouse and creating the Power BI report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4539352d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "print(\"Waiting for semantic model to be ready...\")\n",
        "\n",
        "check_interval = 5  # Interval in seconds between checks\n",
        "max_retries = 60  # Maximum number of retries (5 minutes)\n",
        "\n",
        "retries = 0\n",
        "model_ready = False\n",
        "\n",
        "while retries < max_retries:\n",
        "    try:\n",
        "        df = fabric.list_datasets()\n",
        "        df_filt = df[df[\"Dataset Name\"] == semantic_model_name]\n",
        "        \n",
        "        if not df_filt.empty:\n",
        "            print(f\"‚úì Semantic model found\")\n",
        "            \n",
        "            # Update DirectLake connection\n",
        "            print(\"Updating DirectLake lakehouse connection...\")\n",
        "            labs.directlake.update_direct_lake_model_lakehouse_connection(\n",
        "                dataset=semantic_model_name,\n",
        "                lakehouse=lakehouse\n",
        "            )\n",
        "            print(\"‚úì Lakehouse connection updated\")\n",
        "            \n",
        "            # Create basic report\n",
        "            print(f\"Creating Power BI report '{report_name}'...\")\n",
        "            \n",
        "            # Simple report JSON structure\n",
        "            report_definition = {\n",
        "                \"config\": json.dumps({\n",
        "                    \"version\": \"5.57\",\n",
        "                    \"themeCollection\": {\n",
        "                        \"baseTheme\": {\"name\": \"CY24SU08\", \"version\": \"5.58\", \"type\": 2}\n",
        "                    },\n",
        "                    \"activeSectionIndex\": 0\n",
        "                }),\n",
        "                \"layoutOptimization\": 0,\n",
        "                \"sections\": [\n",
        "                    {\n",
        "                        \"displayName\": \"Migration Overview\",\n",
        "                        \"width\": 1280,\n",
        "                        \"height\": 720,\n",
        "                        \"displayOption\": 1,\n",
        "                        \"visualContainers\": []\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "            \n",
        "            labs_report.create_report_from_reportjson(\n",
        "                report=report_name,\n",
        "                dataset=semantic_model_name,\n",
        "                report_json=report_definition\n",
        "            )\n",
        "            \n",
        "            print(f\"‚úì Report '{report_name}' created successfully\")\n",
        "            print(\"\\n\" + \"=\" * 70)\n",
        "            print(\"‚úÖ SETUP COMPLETE\")\n",
        "            print(\"=\" * 70)\n",
        "            print(f\"Lakehouse: {lakehouse}\")\n",
        "            print(f\"Semantic Model: {semantic_model_name}\")\n",
        "            print(f\"Report: {report_name}\")\n",
        "            print(\"\\nYou can now open the report and build custom visualizations.\")\n",
        "            print(\"=\" * 70)\n",
        "            \n",
        "            model_ready = True\n",
        "            break\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Attempt {retries + 1}/{max_retries}: {str(e)}\")\n",
        "    \n",
        "    if not model_ready:\n",
        "        print(f\"Retrying in {check_interval} seconds... ({retries + 1}/{max_retries})\")\n",
        "        time.sleep(check_interval)\n",
        "        retries += 1\n",
        "\n",
        "if not model_ready:\n",
        "    print(\"‚ùå Max retries reached. Semantic model may not be ready yet.\")\n",
        "    print(\"Please check the workspace and try updating the connection manually.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53f0144e",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook has:\n",
        "1. ‚úÖ Validated prerequisites and permissions\n",
        "2. ‚úÖ Collected comprehensive data from your tenant\n",
        "3. ‚úÖ Analyzed migration blockers and compatibility issues\n",
        "4. ‚úÖ Stored all data in a Fabric Lakehouse\n",
        "5. ‚úÖ Created a semantic model and Power BI report\n",
        "\n",
        "### Next Steps\n",
        "1. **Review the Migration Analysis** output above for any blockers or warnings\n",
        "2. **Open the Power BI Report** to explore your capacity landscape visually\n",
        "3. **Address any blockers** identified in the analysis\n",
        "4. **Plan your migration** strategy based on the findings\n",
        "\n",
        "### Support\n",
        "For questions or issues, please refer to the deployment guide or contact your Fabric administrator."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
