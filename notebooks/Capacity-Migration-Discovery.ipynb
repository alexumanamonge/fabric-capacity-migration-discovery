{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "11063a06",
      "metadata": {},
      "source": [
        "# Microsoft Fabric Capacity Migration Discovery\n",
        "\n",
        "## Overview\n",
        "This notebook performs comprehensive discovery of Power BI Premium/Embedded capacities and identifies potential blockers for migration to Microsoft Fabric capacities.\n",
        "\n",
        "## What This Notebook Does\n",
        "1. ‚úÖ Validates prerequisites (admin access, permissions)\n",
        "2. üìä Collects data from all capacities, workspaces, and items\n",
        "3. üîç Analyzes migration blockers and compatibility issues\n",
        "4. üíæ Stores data in a Fabric Lakehouse\n",
        "5. üìà Creates interactive Power BI report for analysis\n",
        "\n",
        "## Prerequisites\n",
        "- **Tenant Administrator** or **Capacity Administrator** permissions\n",
        "- XMLA Read/Write enabled on capacities\n",
        "- Fabric workspace with Lakehouse creation permissions\n",
        "\n",
        "## Last Updated\n",
        "November 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fcf8b24",
      "metadata": {},
      "source": [
        "## Step 1: Install Required Libraries\n",
        "\n",
        "Install semantic-link and semantic-link-labs for Fabric integration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79da2263",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install semantic-link-labs --upgrade --quiet\n",
        "print(\"‚úì Libraries installed successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c6acd46",
      "metadata": {},
      "source": [
        "## Step 2: Configure Parameters\n",
        "\n",
        "Set the names for your lakehouse, semantic model, and report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dfe138f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration Parameters\n",
        "semantic_model_name = \"Capacity Migration Analysis\"\n",
        "report_name = \"Capacity Migration Report\"\n",
        "lakehouse = \"CapacityMigrationLH\"\n",
        "\n",
        "# Validate parameters\n",
        "if not all([semantic_model_name, report_name, lakehouse]):\n",
        "    raise ValueError(\"‚ùå Error: All parameters (semantic_model_name, report_name, lakehouse) must be set\")\n",
        "\n",
        "print(\"‚úì Configuration validated:\")\n",
        "print(f\"  ‚Ä¢ Lakehouse: {lakehouse}\")\n",
        "print(f\"  ‚Ä¢ Semantic Model: {semantic_model_name}\")\n",
        "print(f\"  ‚Ä¢ Report: {report_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66892f0d",
      "metadata": {},
      "source": [
        "## Step 3: Import Libraries and Validate Permissions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d0474d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sempy\n",
        "import sempy.fabric as fabric\n",
        "import sempy_labs as labs\n",
        "import sempy_labs.admin as labs_admin\n",
        "import sempy_labs.report as labs_report\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"‚úì Libraries imported successfully\")\n",
        "\n",
        "# Validate tenant admin access\n",
        "try:\n",
        "    test_capacities = labs_admin._capacities._list_capacities_meta()\n",
        "    print(f\"‚úì Tenant admin access confirmed\")\n",
        "    print(f\"  ‚Ä¢ Found {len(test_capacities)} capacities accessible\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: Requires tenant/capacity admin permissions\")\n",
        "    print(f\"  ‚Ä¢ Details: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d7792c",
      "metadata": {},
      "source": [
        "## Step 4: Create or Verify Lakehouse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "050eaa13",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if Lakehouse exists, otherwise create it\n",
        "try:\n",
        "    df_items = fabric.list_items()\n",
        "    lakehouse_exists = df_items['Display Name'].eq(lakehouse).any()\n",
        "    \n",
        "    if lakehouse_exists:\n",
        "        lhid = df_items[\n",
        "            (df_items['Display Name'].eq(lakehouse)) & \n",
        "            (df_items['Type'].eq('Lakehouse'))\n",
        "        ].iloc[0, 0]\n",
        "        print(f\"‚úì Lakehouse '{lakehouse}' already exists (ID: {lhid})\")\n",
        "    else:\n",
        "        print(f\"Creating new Lakehouse '{lakehouse}'...\")\n",
        "        lhid = fabric.create_lakehouse(lakehouse)\n",
        "        print(f\"‚úì Lakehouse created successfully (ID: {lhid})\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creating/verifying Lakehouse: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35655873",
      "metadata": {},
      "source": [
        "## Step 5: Collect Capacity Data\n",
        "\n",
        "Gathering all capacity information including SKU, region, and state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5546bd71",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Collecting capacity data...\")\n",
        "\n",
        "try:\n",
        "    # Get all capacities\n",
        "    df_capacities = labs_admin._capacities._list_capacities_meta()\n",
        "    \n",
        "    # Convert Admins column to string to avoid type conflicts\n",
        "    if 'Admins' in df_capacities.columns:\n",
        "        df_capacities['Admins'] = df_capacities['Admins'].apply(\n",
        "            lambda x: json.dumps(x) if x is not None else None\n",
        "        )\n",
        "    \n",
        "    # Add a record for non-premium workspaces\n",
        "    new_record = pd.DataFrame([{\n",
        "        \"Capacity Id\": \"-1\",\n",
        "        \"Capacity Name\": \"Non Premium (Shared)\",\n",
        "        \"Sku\": \"Shared\",\n",
        "        \"Region\": \"N/A\",\n",
        "        \"State\": \"Active\",\n",
        "        \"Admins\": json.dumps([\"N/A\"])\n",
        "    }])\n",
        "    \n",
        "    df_capacities = pd.concat([df_capacities, new_record], ignore_index=True)\n",
        "    \n",
        "    print(f\"‚úì Collected {len(df_capacities) - 1} Premium/Fabric capacities\")\n",
        "    print(f\"\\nCapacity Summary:\")\n",
        "    print(df_capacities.groupby('Sku').size().to_string())\n",
        "    \n",
        "    # Save to Lakehouse\n",
        "    labs.save_as_delta_table(\n",
        "        dataframe=df_capacities,\n",
        "        delta_table_name=\"Capacities\",\n",
        "        write_mode=\"overwrite\",\n",
        "        lakehouse=lakehouse\n",
        "    )\n",
        "    print(\"\\n‚úì Capacities data saved to Lakehouse\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error collecting capacity data: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b01145d",
      "metadata": {},
      "source": [
        "## Step 6: Collect Workspace Data\n",
        "\n",
        "Gathering all workspace information and capacity assignments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "079b501e",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Collecting workspace data...\")\n",
        "\n",
        "try:\n",
        "    # Get all workspaces\n",
        "    all_workspaces_df = labs_admin.list_workspaces()\n",
        "    \n",
        "    # Fill null Capacity Ids with -1 (non-premium)\n",
        "    all_workspaces_df['Capacity Id'] = all_workspaces_df['Capacity Id'].fillna(\"-1\")\n",
        "    \n",
        "    print(f\"‚úì Collected {len(all_workspaces_df)} workspaces\")\n",
        "    \n",
        "    # Workspace breakdown\n",
        "    premium_count = (all_workspaces_df['Capacity Id'] != \"-1\").sum()\n",
        "    shared_count = (all_workspaces_df['Capacity Id'] == \"-1\").sum()\n",
        "    \n",
        "    print(f\"  ‚Ä¢ Premium/Fabric workspaces: {premium_count}\")\n",
        "    print(f\"  ‚Ä¢ Shared workspaces: {shared_count}\")\n",
        "    \n",
        "    # Save to Lakehouse\n",
        "    labs.save_as_delta_table(\n",
        "        dataframe=all_workspaces_df,\n",
        "        delta_table_name=\"Workspaces\",\n",
        "        write_mode=\"overwrite\",\n",
        "        lakehouse=lakehouse\n",
        "    )\n",
        "    print(\"‚úì Workspaces data saved to Lakehouse\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error collecting workspace data: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "197f2463",
      "metadata": {},
      "source": [
        "## Step 7: Collect Workspace Items\n",
        "\n",
        "Gathering all items (reports, datasets, dataflows, etc.) across all workspaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b36bd287",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Collecting workspace items...\")\n",
        "\n",
        "try:\n",
        "    # Get all items across all workspaces\n",
        "    df_items = labs_admin.list_items()\n",
        "    \n",
        "    print(f\"‚úì Collected {len(df_items)} workspace items\")\n",
        "    print(f\"\\nItem Type Breakdown:\")\n",
        "    print(df_items['Type'].value_counts().to_string())\n",
        "    \n",
        "    # Save to Lakehouse\n",
        "    labs.save_as_delta_table(\n",
        "        dataframe=df_items,\n",
        "        delta_table_name=\"WorkspaceItems\",\n",
        "        write_mode=\"overwrite\",\n",
        "        lakehouse=lakehouse\n",
        "    )\n",
        "    print(\"\\n‚úì Workspace items data saved to Lakehouse\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error collecting workspace items: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9680d1c",
      "metadata": {},
      "source": [
        "## Step 8: Collect Semantic Model Data\n",
        "\n",
        "Gathering detailed information about all semantic models (datasets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e973f4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Collecting semantic model data...\")\n",
        "\n",
        "try:\n",
        "    # Get all semantic models\n",
        "    df_semantic_models = labs_admin.list_datasets()\n",
        "    \n",
        "    # Convert complex fields to JSON strings for storage\n",
        "    df_semantic_models[\"Upstream Datasets\"] = df_semantic_models[\"Upstream Datasets\"].apply(\n",
        "        lambda x: json.dumps(x) if x and x != \"[]\" else None\n",
        "    )\n",
        "    df_semantic_models[\"Users\"] = df_semantic_models[\"Users\"].apply(\n",
        "        lambda x: json.dumps(x) if x and x != \"[]\" else None\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úì Collected {len(df_semantic_models)} semantic models\")\n",
        "    \n",
        "    # Storage mode breakdown\n",
        "    if 'Target Storage Mode' in df_semantic_models.columns:\n",
        "        print(f\"\\nStorage Mode Breakdown:\")\n",
        "        print(df_semantic_models['Target Storage Mode'].value_counts().to_string())\n",
        "    \n",
        "    # Save to Lakehouse\n",
        "    labs.save_as_delta_table(\n",
        "        dataframe=df_semantic_models,\n",
        "        delta_table_name=\"SemanticModels\",\n",
        "        write_mode=\"overwrite\",\n",
        "        lakehouse=lakehouse\n",
        "    )\n",
        "    print(\"\\n‚úì Semantic models data saved to Lakehouse\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error collecting semantic model data: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc55ac47",
      "metadata": {},
      "source": [
        "## Step 9: Migration Blocker Analysis\n",
        "\n",
        "Analyzing potential blockers for migration from Premium to Fabric capacities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8395af6",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"MIGRATION READINESS ASSESSMENT\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "\n",
        "blockers = []\n",
        "warnings = []\n",
        "info_items = []\n",
        "\n",
        "# ============================================\n",
        "# 1. Check for Embedded (EM) SKUs\n",
        "# ============================================\n",
        "em_skus = df_capacities[\n",
        "    df_capacities['Sku'].str.startswith('EM', na=False) |\n",
        "    df_capacities['Sku'].str.startswith('A', na=False)  # Azure A SKUs\n",
        "]\n",
        "if not em_skus.empty:\n",
        "    for _, cap in em_skus.iterrows():\n",
        "        if cap['Sku'].startswith('EM'):\n",
        "            blockers.append(\n",
        "                f\"Embedded capacity '{cap['Capacity Name']}' (SKU: {cap['Sku']}) - \"\n",
        "                f\"EM SKUs not supported in Fabric. Migrate to F-SKUs.\"\n",
        "            )\n",
        "        elif cap['Sku'].startswith('A'):\n",
        "            info_items.append(\n",
        "                f\"Azure capacity '{cap['Capacity Name']}' (SKU: {cap['Sku']}) - \"\n",
        "                f\"Azure Embedded can migrate to Fabric F-SKUs.\"\n",
        "            )\n",
        "\n",
        "# ============================================\n",
        "# 2. Check for Premium P SKUs (ready for migration)\n",
        "# ============================================\n",
        "p_skus = df_capacities[df_capacities['Sku'].str.startswith('P', na=False)]\n",
        "if not p_skus.empty:\n",
        "    info_items.append(\n",
        "        f\"{len(p_skus)} Premium P-SKU capacities found - Ready for Fabric migration\"\n",
        "    )\n",
        "\n",
        "# ============================================\n",
        "# 3. Check for cross-region workspaces\n",
        "# ============================================\n",
        "capacity_regions = df_capacities[\n",
        "    df_capacities['Region'] != 'N/A'\n",
        "]['Region'].unique()\n",
        "\n",
        "if len(capacity_regions) > 1:\n",
        "    warnings.append(\n",
        "        f\"Multiple regions detected: {', '.join(capacity_regions)}. \"\n",
        "        f\"Fabric capacities are region-specific. Plan migrations within same region.\"\n",
        "    )\n",
        "\n",
        "# ============================================\n",
        "# 4. Check for Dataflows Gen1\n",
        "# ============================================\n",
        "dataflows_gen1 = df_items[df_items['Type'] == 'Dataflow']\n",
        "if not dataflows_gen1.empty:\n",
        "    warnings.append(\n",
        "        f\"{len(dataflows_gen1)} Dataflow Gen1 artifacts found. \"\n",
        "        f\"Consider upgrading to Dataflow Gen2 or Data Pipelines in Fabric.\"\n",
        "    )\n",
        "\n",
        "# ============================================\n",
        "# 5. Check for Paginated Reports\n",
        "# ============================================\n",
        "paginated = df_items[df_items['Type'] == 'PaginatedReport']\n",
        "if not paginated.empty:\n",
        "    warnings.append(\n",
        "        f\"{len(paginated)} Paginated Reports found. \"\n",
        "        f\"Ensure Fabric capacity has paginated report workload enabled.\"\n",
        "    )\n",
        "\n",
        "# ============================================\n",
        "# 6. Check for Large Models (Premium Files)\n",
        "# ============================================\n",
        "if 'Target Storage Mode' in df_semantic_models.columns:\n",
        "    large_models = df_semantic_models[\n",
        "        df_semantic_models['Target Storage Mode'] == 'PremiumFiles'\n",
        "    ]\n",
        "    if not large_models.empty:\n",
        "        warnings.append(\n",
        "            f\"{len(large_models)} Large Models (>10GB) detected. \"\n",
        "            f\"Verify target Fabric capacity size supports these models.\"\n",
        "        )\n",
        "\n",
        "# ============================================\n",
        "# 7. Check for Inactive Workspaces\n",
        "# ============================================\n",
        "if 'State' in all_workspaces_df.columns:\n",
        "    inactive = all_workspaces_df[all_workspaces_df['State'] != 'Active']\n",
        "    if not inactive.empty:\n",
        "        warnings.append(\n",
        "            f\"{len(inactive)} inactive workspaces found (States: {inactive['State'].unique()}). \"\n",
        "            f\"Review and clean up before migration.\"\n",
        "        )\n",
        "\n",
        "# ============================================\n",
        "# 8. Check for Models with RLS\n",
        "# ============================================\n",
        "if 'Is Effective Identity Required' in df_semantic_models.columns:\n",
        "    models_with_rls = df_semantic_models[\n",
        "        df_semantic_models['Is Effective Identity Required'] == True\n",
        "    ]\n",
        "    if not models_with_rls.empty:\n",
        "        info_items.append(\n",
        "            f\"{len(models_with_rls)} semantic models with Row-Level Security (RLS). \"\n",
        "            f\"Test RLS behavior after migration, especially with DirectLake.\"\n",
        "        )\n",
        "\n",
        "# ============================================\n",
        "# 9. Check for Dashboards\n",
        "# ============================================\n",
        "dashboards = df_items[df_items['Type'] == 'Dashboard']\n",
        "if not dashboards.empty:\n",
        "    info_items.append(\n",
        "        f\"{len(dashboards)} Dashboards found. Dashboards migrate with their tiles and data sources.\"\n",
        "    )\n",
        "\n",
        "# ============================================\n",
        "# 10. Check for Deployment Pipelines\n",
        "# ============================================\n",
        "if 'Pipeline Id' in all_workspaces_df.columns:\n",
        "    pipelines = all_workspaces_df[all_workspaces_df['Pipeline Id'].notna()]\n",
        "    if not pipelines.empty:\n",
        "        info_items.append(\n",
        "            f\"{len(pipelines)} workspaces use Deployment Pipelines. \"\n",
        "            f\"Pipelines are supported in Fabric - verify configuration post-migration.\"\n",
        "        )\n",
        "\n",
        "# ============================================\n",
        "# Display Results\n",
        "# ============================================\n",
        "print(\"\\n\")\n",
        "\n",
        "if blockers:\n",
        "    print(\"üõë CRITICAL BLOCKERS (Must resolve before migration):\")\n",
        "    print(\"=\" * 70)\n",
        "    for i, blocker in enumerate(blockers, 1):\n",
        "        print(f\"{i}. {blocker}\")\n",
        "    print()\n",
        "\n",
        "if warnings:\n",
        "    print(\"‚ö†Ô∏è  WARNINGS (Review and plan accordingly):\")\n",
        "    print(\"=\" * 70)\n",
        "    for i, warning in enumerate(warnings, 1):\n",
        "        print(f\"{i}. {warning}\")\n",
        "    print()\n",
        "\n",
        "if info_items:\n",
        "    print(\"‚ÑπÔ∏è  INFORMATIONAL (For your awareness):\")\n",
        "    print(\"=\" * 70)\n",
        "    for i, info in enumerate(info_items, 1):\n",
        "        print(f\"{i}. {info}\")\n",
        "    print()\n",
        "\n",
        "if not blockers and not warnings:\n",
        "    print(\"‚úÖ MIGRATION READY\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"No critical blockers or warnings detected.\")\n",
        "    print(\"Your environment appears ready for Fabric migration.\")\n",
        "    print()\n",
        "\n",
        "# ============================================\n",
        "# Save Analysis Results\n",
        "# ============================================\n",
        "analysis_results = pd.DataFrame({\n",
        "    'Analysis Date': [datetime.now()],\n",
        "    'Total Capacities': [len(df_capacities) - 1],\n",
        "    'Total Workspaces': [len(all_workspaces_df)],\n",
        "    'Total Items': [len(df_items)],\n",
        "    'Total Models': [len(df_semantic_models)],\n",
        "    'Blocker Count': [len(blockers)],\n",
        "    'Warning Count': [len(warnings)],\n",
        "    'Blockers': [json.dumps(blockers)],\n",
        "    'Warnings': [json.dumps(warnings)],\n",
        "    'Info': [json.dumps(info_items)]\n",
        "})\n",
        "\n",
        "labs.save_as_delta_table(\n",
        "    dataframe=analysis_results,\n",
        "    delta_table_name=\"MigrationAnalysis\",\n",
        "    write_mode=\"overwrite\",\n",
        "    lakehouse=lakehouse\n",
        ")\n",
        "\n",
        "print(\"‚úì Analysis results saved to Lakehouse (Table: MigrationAnalysis)\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9547cd04",
      "metadata": {},
      "source": [
        "## Step 10: Verify Data in Lakehouse\n",
        "\n",
        "All capacity discovery data has been saved to Delta tables. Let's verify the data is ready for reporting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a68081f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Verifying Delta tables in Lakehouse...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# List all tables created\n",
        "tables_to_verify = [\"Capacities\", \"Workspaces\", \"WorkspaceItems\", \"SemanticModels\"]\n",
        "\n",
        "for table_name in tables_to_verify:\n",
        "    try:\n",
        "        # Read table and get row count\n",
        "        df = spark.read.table(f\"{lakehouse}.{table_name}\")\n",
        "        row_count = df.count()\n",
        "        col_count = len(df.columns)\n",
        "        \n",
        "        print(f\"‚úì {table_name:20} | {row_count:5} rows | {col_count:3} columns\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {table_name:20} | Error: {str(e)}\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n‚úÖ DATA COLLECTION COMPLETE\")\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"NEXT STEPS - Create DirectLake Semantic Model (Manual)\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nMicrosoft Fabric's recommended approach for DirectLake semantic models\")\n",
        "print(\"is to create them via the UI (not programmatically).\\n\")\n",
        "print(\"Follow these steps:\\n\")\n",
        "print(\"1. Navigate to your Lakehouse in the Fabric workspace\")\n",
        "print(f\"   Lakehouse name: {lakehouse}\\n\")\n",
        "print(\"2. Click the 'New semantic model' button at the top\")\n",
        "print(\"   (alternatively: ‚ãÆ menu ‚Üí New semantic model)\\n\")\n",
        "print(\"3. In the dialog that appears:\")\n",
        "print(\"   a. Name: 'Capacity Migration Discovery Model'\")\n",
        "print(\"   b. Select these 4 tables:\")\n",
        "print(\"      ‚òë Capacities\")\n",
        "print(\"      ‚òë Workspaces\")\n",
        "print(\"      ‚òë WorkspaceItems\")\n",
        "print(\"      ‚òë SemanticModels\")\n",
        "print(\"   c. Click 'Confirm'\\n\")\n",
        "print(\"4. The DirectLake semantic model will be created automatically\")\n",
        "print(\"   (takes ~10-30 seconds)\\n\")\n",
        "print(\"5. Click 'Open data model' to add relationships and measures:\\n\")\n",
        "print(\"   Add these relationships:\")\n",
        "print(\"   ‚Ä¢ Workspaces[Id] ‚Üí Capacities[Id] (Many-to-One)\")\n",
        "print(\"   ‚Ä¢ WorkspaceItems[WorkspaceId] ‚Üí Workspaces[Id] (Many-to-One)\")\n",
        "print(\"   ‚Ä¢ SemanticModels[WorkspaceId] ‚Üí Workspaces[Id] (Many-to-One)\\n\")\n",
        "print(\"   Add these DAX measures (optional but recommended):\")\n",
        "print(\"   ‚Ä¢ Total Capacities = COUNTROWS(Capacities)\")\n",
        "print(\"   ‚Ä¢ Total Workspaces = COUNTROWS(Workspaces)\")\n",
        "print(\"   ‚Ä¢ Total Items = COUNTROWS(WorkspaceItems)\")\n",
        "print(\"   ‚Ä¢ Total Datasets = COUNTROWS(SemanticModels)\")\n",
        "print(\"   ‚Ä¢ Blocked Items = COUNTROWS(FILTER(WorkspaceItems, [Has Migration Blocker] = TRUE))\\n\")\n",
        "print(\"6. Create a Power BI report:\")\n",
        "print(\"   a. From the semantic model, click 'Create report'\")\n",
        "print(\"   b. Add visuals to analyze:\")\n",
        "print(\"      - Capacity distribution (pie chart)\")\n",
        "print(\"      - Migration blockers by category (stacked bar)\")\n",
        "print(\"      - Workspace and item counts (cards)\")\n",
        "print(\"      - Items with migration blockers (table)\\n\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüí° TIP: This manual process takes ~2 minutes and is the\")\n",
        "print(\"   official Microsoft-recommended approach for DirectLake models.\")\n",
        "print(\"   See: https://learn.microsoft.com/fabric/fundamentals/direct-lake-create-lakehouse\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53f0144e",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook has:\n",
        "1. ‚úÖ Validated prerequisites and permissions\n",
        "2. ‚úÖ Collected comprehensive data from your tenant\n",
        "3. ‚úÖ Analyzed migration blockers and compatibility issues\n",
        "4. ‚úÖ Stored all data in a Fabric Lakehouse\n",
        "5. ‚úÖ Created a semantic model and Power BI report\n",
        "\n",
        "### Next Steps\n",
        "1. **Review the Migration Analysis** output above for any blockers or warnings\n",
        "2. **Open the Power BI Report** to explore your capacity landscape visually\n",
        "3. **Address any blockers** identified in the analysis\n",
        "4. **Plan your migration** strategy based on the findings\n",
        "\n",
        "### Support\n",
        "For questions or issues, please refer to the deployment guide or contact your Fabric administrator."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
